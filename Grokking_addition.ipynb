{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1K2Sg5JYb4fuwCyVtvOBKKNp7ZN3Uh5hp",
      "authorship_tag": "ABX9TyNWebzOam6sORxF2lY27KUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/geometric-grokking/blob/main/Grokking_addition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1.0\n",
        "EPOCHS = 40000\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 5\n",
        "\n",
        "#torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ==========================================\n",
        "# BULLETPROOF DETERMINISM\n",
        "# ==========================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"Locks down all sources of randomness for 100% reproducibility.\"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # Required for strict cuDNN determinism\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    set_seed(seed) # Ensure dataset split is also deterministic per seed\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# 1. STANDARD & SPHERICAL TRANSFORMER\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim, normalize_hiddens=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.normalize_hiddens = normalize_hiddens\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "def inject_fourier_bias(model, p):\n",
        "    with torch.no_grad():\n",
        "        key_freqs = [14, 35, 41, 42, 52]\n",
        "        for i, k in enumerate(key_freqs):\n",
        "            for x in range(p):\n",
        "                val = 2 * math.pi * k * x / p\n",
        "                model.tok_embed.weight[x, 2 * i] = math.cos(val)\n",
        "                model.tok_embed.weight[x, 2 * i + 1] = math.sin(val)\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE STRICT PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# TRAINING LOGIC\n",
        "# ==========================================\n",
        "def train_model(model, name, train_x, train_y, test_x, test_y, epochs):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(epochs), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == epochs - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else f\">{epochs}\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING (1x3 Grid, Paper Ready)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: The 3 Quadrants of Generalization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Generate Dataset Deterministically\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    # Format: (Name, Model, Epochs)\n",
        "    models = []\n",
        "\n",
        "    # 2. Standard Model (Baseline) - Needs more time to grok\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Standard\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False), 80000))\n",
        "\n",
        "    # 3. Fourier Init Model (Treasure Map)\n",
        "    set_seed(SEED)\n",
        "    f_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)\n",
        "    inject_fourier_bias(f_model, P)\n",
        "    models.append((\"Fourier Init\", f_model, 40000))\n",
        "\n",
        "    # 4. Spherical Norm (L2 Straitjacket) - Learns instantly\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Spherical Norm\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, True), 10000))\n",
        "\n",
        "    results = {}\n",
        "    for name, model, train_epochs in models:\n",
        "        model = model.to(DEVICE)\n",
        "        results[name] = train_model(model, name, tr_x, tr_y, te_x, te_y, train_epochs)\n",
        "\n",
        "        # Save logs to Drive\n",
        "        safe_name = name.replace(\" \", \"_\").replace(\"(|z|=1)\", \"z1\")\n",
        "        save_path = os.path.join(SAVE_DIR, f\"{safe_name}_seed{SEED}.json\")\n",
        "        with open(save_path, \"w\") as f:\n",
        "            json.dump(results[name][\"history\"], f)\n",
        "        print(f\"Saved {name} logs to {save_path}\")\n",
        "\n",
        "    # --- PLOTTING 1x3 GRID (PAPER READY) ---\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, (name, res) in enumerate(results.items()):\n",
        "        hist = res[\"history\"]\n",
        "\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "        axs[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "        title_text = f\"{name} Transformer\\nGeneralized at: {res['grok_epoch']}\"\n",
        "        axs[i].set_title(title_text, fontsize=16, pad=10)\n",
        "        axs[i].set_xlabel(\"Epochs\", fontsize=14)\n",
        "        axs[i].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "        axs[i].tick_params(axis='both', which='major', labelsize=12)\n",
        "        axs[i].grid(True, alpha=0.3)\n",
        "        axs[i].legend(loc=\"lower right\", fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save high-res plot to Drive\n",
        "    plot_path = os.path.join(SAVE_DIR, f\"grokking_1x3_grid_seed{SEED}.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✅ Saved publication-ready plot to: {plot_path}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tWDRCTaNPA7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs'\n",
        "model_names = [\"Standard\", \"Fourier_Init\", \"Spherical_Norm\"]\n",
        "\n",
        "results_data = []\n",
        "\n",
        "print(\"Analyzing grokking moments across all seeds...\\n\")\n",
        "\n",
        "for model in model_names:\n",
        "    files = glob.glob(f\"{SAVE_DIR}/{model}_seed*.json\")\n",
        "    if not files:\n",
        "        continue\n",
        "\n",
        "    grok_epochs = []\n",
        "    did_not_grok_count = 0\n",
        "\n",
        "    for f in files:\n",
        "        data = json.load(open(f))\n",
        "        test_accs = data[\"test_acc\"]\n",
        "        epochs = data[\"epochs\"]\n",
        "\n",
        "        # Find the first epoch where test_acc > 0.95\n",
        "        grok_epoch = None\n",
        "        for epoch, acc in zip(epochs, test_accs):\n",
        "            if acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                break\n",
        "\n",
        "        if grok_epoch is not None:\n",
        "            grok_epochs.append(grok_epoch)\n",
        "        else:\n",
        "            did_not_grok_count += 1\n",
        "\n",
        "    if grok_epochs:\n",
        "        results_data.append({\n",
        "            \"Model\": model.replace(\"_\", \" \"),\n",
        "            \"Seeds Run\": len(files),\n",
        "            \"Successful Groks\": len(grok_epochs),\n",
        "            \"Earliest Grok\": np.min(grok_epochs),\n",
        "            \"Latest Grok\": np.max(grok_epochs),\n",
        "            \"Average Grok\": int(np.mean(grok_epochs)),\n",
        "            \"Std Dev\": int(np.std(grok_epochs))\n",
        "        })\n",
        "\n",
        "if results_data:\n",
        "    # Use Pandas to print a clean, paper-ready table\n",
        "    df = pd.DataFrame(results_data)\n",
        "    print(df.to_markdown(index=False))\n",
        "else:\n",
        "    print(\"No log files found yet. Make sure training has finished!\")"
      ],
      "metadata": {
        "id": "6Oa4oUsjhSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "\n",
        "# Make sure the directory exists\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "env_log_path = os.path.join(SAVE_DIR, \"environment_info.txt\")\n",
        "\n",
        "with open(env_log_path, \"w\") as f:\n",
        "    f.write(f\"--- Grokking Experiment Environment Log ---\\n\")\n",
        "    f.write(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    # 1. Python Version\n",
        "    f.write(\"=== Python Version ===\\n\")\n",
        "    f.write(sys.version + \"\\n\\n\")\n",
        "\n",
        "    # 2. PyTorch & GPU Info\n",
        "    f.write(\"=== PyTorch & CUDA ===\\n\")\n",
        "    f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
        "    f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
        "    if torch.cuda.is_available():\n",
        "        f.write(f\"CUDA Built-in Version: {torch.version.cuda}\\n\")\n",
        "        f.write(f\"cuDNN Version: {torch.backends.cudnn.version()}\\n\")\n",
        "        f.write(f\"GPU Model: {torch.cuda.get_device_name(0)}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    # 3. Full NVIDIA-SMI Output\n",
        "    f.write(\"=== NVIDIA-SMI ===\\n\")\n",
        "    try:\n",
        "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
        "        f.write(nvidia_smi + \"\\n\")\n",
        "    except Exception as e:\n",
        "        f.write(f\"Could not retrieve nvidia-smi: {e}\\n\\n\")\n",
        "\n",
        "    # 4. Pip Freeze (All installed libraries)\n",
        "    f.write(\"=== Installed PIP Packages ===\\n\")\n",
        "    try:\n",
        "        pip_freeze = subprocess.check_output(\"pip freeze\", shell=True).decode()\n",
        "        f.write(pip_freeze + \"\\n\")\n",
        "    except Exception as e:\n",
        "        f.write(f\"Could not retrieve pip freeze: {e}\\n\")\n",
        "\n",
        "print(f\"✅ Environment successfully logged to: {env_log_path}\")"
      ],
      "metadata": {
        "id": "SiNdMfEpJwyv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}