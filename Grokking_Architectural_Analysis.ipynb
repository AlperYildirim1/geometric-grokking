{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM5ZnQn5aitaPavnc2xlfuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/geometric-grokking/blob/main/Grokking_Architectural_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T_R74LnbyaE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Grokking Experiment: Phase Attention vs Standard Attention\n",
        "===========================================================\n",
        "Task: Modular addition mod P=113\n",
        "Setup: Evaluates 3 distinct topological architectures on the grokking threshold.\n",
        "  1. Standard Transformer (Absolute PE)\n",
        "  2. RoPE Transformer (Rotary PE)\n",
        "  3. Phase Transformer (|z|=1 enforced, protected optimization, pure wave interference)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113                  # Prime modulus\n",
        "FRAC_TRAIN = 0.3         # 30% train, 70% test\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 5e-3\n",
        "WEIGHT_DECAY = 1.0       # High decay to force grokking\n",
        "EPOCHS = 20000\n",
        "LOG_EVERY = 50\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_pairs = all_pairs[:n_train]\n",
        "    test_pairs = all_pairs[n_train:]\n",
        "\n",
        "    def pairs_to_tensors(pairs):\n",
        "        xs = torch.tensor([[a, b, p] for a, b in pairs], dtype=torch.long)\n",
        "        ys = torch.tensor([(a + b) % p for a, b in pairs], dtype=torch.long)\n",
        "        return xs, ys\n",
        "\n",
        "    train_x, train_y = pairs_to_tensors(train_pairs)\n",
        "    test_x, test_y = pairs_to_tensors(test_pairs)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# BASELINE 1: STANDARD TRANSFORMER (Nanda)\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn_out = self.W_O((attn @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# BASELINE 2: ROPE TRANSFORMER\n",
        "# ==========================================\n",
        "class RoPETransformer(StandardTransformer):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__(p, d_model, num_heads, mlp_dim)\n",
        "        # Remove absolute pos embed, replace with RoPE frequencies\n",
        "        self.pos_embed = None\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_head, 2).float() / self.d_head))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def apply_rope(self, x, seq_len):\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # Align to [1, 1, L, d_head] to broadcast across Batch and Heads\n",
        "        cos = emb.cos().unsqueeze(0).unsqueeze(1)\n",
        "        sin = emb.sin().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        rotated = torch.cat((-x2, x1), dim=-1)\n",
        "        return (x * cos) + (rotated * sin)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        h = self.tok_embed(x)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        Q = self.apply_rope(Q, L)\n",
        "        K = self.apply_rope(K, L)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn_out = self.W_O((attn @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# THE MENACE: PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        # Initialize W_O very small so Superposition starts as Identity\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "\n",
        "        # DO NOT SNAP HERE. Return scaled vector for superposition.\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        # STRICT IDENTITY INIT: Forces angle=0.0 at Step 0\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        # STRICT IDENTITY INIT: Don't force random phase shifts at Step 0\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "\n",
        "        # Positional rotation\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        # Superposition Residual: Add, THEN snap\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "\n",
        "        # Multiplicative Residual: Rotate\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# OPTIMIZER ROUTER & TRAINING LOGIC\n",
        "# ==========================================\n",
        "def create_optimizer(model, lr, weight_decay, is_phase_model=False):\n",
        "    if not is_phase_model:\n",
        "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
        "\n",
        "    decay_params, no_decay_params = [], []\n",
        "    protected_modules = ['tok_embed_real', 'tok_embed_imag', 'W_Q', 'W_K', 'W_V']\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad: continue\n",
        "        if any(p in name for p in protected_modules):\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    return torch.optim.AdamW([\n",
        "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "    ], lr=lr, betas=(0.9, 0.98))\n",
        "\n",
        "def train_model(model, name, is_phase, train_x, train_y, test_x, test_y):\n",
        "    optimizer = create_optimizer(model, LR, WEIGHT_DECAY, is_phase)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Live Gradient Tracking\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')).item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\",\n",
        "                'te_loss': f\"{test_loss:.3f}\",\n",
        "                '|g|': f\"{grad_norm:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} GROKKED at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return grok_epoch if grok_epoch else \">20000\"\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(SEED)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: Magnitude vs. Phase Physics\")\n",
        "    print(f\"Modulus P={P} | d_model={D_MODEL} | wd={WEIGHT_DECAY} | Device={DEVICE}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    models = [\n",
        "        (\"Standard Transformer\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM), False),\n",
        "        (\"RoPE Transformer\", RoPETransformer(P, D_MODEL, NUM_HEADS, MLP_DIM), False),\n",
        "        (\"Phase Transformer\", PhaseTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM), True)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for name, model, is_phase in models:\n",
        "        model = model.to(DEVICE)\n",
        "        set_seed(SEED) # Reset seed before each run for fair init\n",
        "        results[name] = train_model(model, name, is_phase, tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"{'MODEL TOPOLOGY':<25} | {'EPOCHS TO GROK (>95% Acc)'}\")\n",
        "    print(\"-\" * 60)\n",
        "    for name, grok_ep in results.items():\n",
        "        print(f\"{name:<25} | {grok_ep}\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a 2x2 grid for deep mechanistic comparison\n",
        "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Extract histories\n",
        "std_hist = results[\"Standard Transformer\"][\"history\"]\n",
        "rope_hist = results[\"RoPE Transformer\"][\"history\"]\n",
        "phase_hist = results[\"Phase Transformer\"][\"history\"]\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 1: The Standard Memorization Trap\n",
        "# ==========================================\n",
        "axs[0, 0].plot(std_hist[\"epochs\"], std_hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2)\n",
        "axs[0, 0].plot(std_hist[\"epochs\"], std_hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2)\n",
        "axs[0, 0].fill_between(std_hist[\"epochs\"], std_hist[\"train_acc\"], std_hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "axs[0, 0].set_title(\"Standard Transformer (Abs PE)\\nNotice the massive memorization gap\", fontsize=14, pad=10)\n",
        "axs[0, 0].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "axs[0, 0].grid(True, alpha=0.3)\n",
        "axs[0, 0].legend(loc=\"lower right\")\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 2: The Phase Physics Bypass\n",
        "# ==========================================\n",
        "axs[0, 1].plot(phase_hist[\"epochs\"], phase_hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2)\n",
        "axs[0, 1].plot(phase_hist[\"epochs\"], phase_hist[\"test_acc\"], label=\"Test Acc\", color=\"#2ca02c\", linewidth=2)\n",
        "axs[0, 1].fill_between(phase_hist[\"epochs\"], phase_hist[\"train_acc\"], phase_hist[\"test_acc\"], color='green', alpha=0.1)\n",
        "axs[0, 1].set_title(\"Phase Transformer (|z|=1)\\nMemorization is physically blocked\", fontsize=14, pad=10)\n",
        "axs[0, 1].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "axs[0, 1].grid(True, alpha=0.3)\n",
        "axs[0, 1].legend(loc=\"lower right\")\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 3: Head-to-Head Test Accuracy\n",
        "# ==========================================\n",
        "axs[1, 0].plot(std_hist[\"epochs\"], std_hist[\"test_acc\"], label=\"Standard (Abs PE)\", color=\"#d62728\", linestyle=\"--\", alpha=0.8)\n",
        "axs[1, 0].plot(rope_hist[\"epochs\"], rope_hist[\"test_acc\"], label=\"RoPE Baseline\", color=\"#ff7f0e\", linewidth=2)\n",
        "axs[1, 0].plot(phase_hist[\"epochs\"], phase_hist[\"test_acc\"], label=\"Phase (|z|=1)\", color=\"#2ca02c\", linewidth=2.5)\n",
        "axs[1, 0].set_title(\"Test Accuracy Comparison\", fontsize=14, pad=10)\n",
        "axs[1, 0].set_xlabel(\"Training Epochs\", fontsize=12)\n",
        "axs[1, 0].set_ylabel(\"Test Accuracy\", fontsize=12)\n",
        "axs[1, 0].grid(True, alpha=0.3)\n",
        "axs[1, 0].legend(loc=\"lower right\")\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 4: Head-to-Head Test Loss (Log Scale)\n",
        "# ==========================================\n",
        "axs[1, 1].plot(std_hist[\"epochs\"], std_hist[\"test_loss\"], label=\"Standard\", color=\"#d62728\", linestyle=\"--\", alpha=0.8)\n",
        "axs[1, 1].plot(rope_hist[\"epochs\"], rope_hist[\"test_loss\"], label=\"RoPE\", color=\"#ff7f0e\", linewidth=2)\n",
        "axs[1, 1].plot(phase_hist[\"epochs\"], phase_hist[\"test_loss\"], label=\"Phase\", color=\"#2ca02c\", linewidth=2.5)\n",
        "axs[1, 1].set_title(\"Test Loss Trajectory (Log Scale)\", fontsize=14, pad=10)\n",
        "axs[1, 1].set_xlabel(\"Training Epochs\", fontsize=12)\n",
        "axs[1, 1].set_ylabel(\"Cross Entropy Loss\", fontsize=12)\n",
        "axs[1, 1].set_yscale(\"log\")\n",
        "axs[1, 1].grid(True, alpha=0.3)\n",
        "axs[1, 1].legend(loc=\"upper right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DAtJLZv2et6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3                # STRICTLY NANDA'S LR\n",
        "WEIGHT_DECAY = 1.0       # STRICTLY NANDA'S DECAY\n",
        "EPOCHS = 35000           # Increased slightly to give 1e-3 Standard time to grok\n",
        "LOG_EVERY = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION (FULL BATCH)\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_pairs = all_pairs[:n_train]\n",
        "    test_pairs = all_pairs[n_train:]\n",
        "\n",
        "    def pairs_to_tensors(pairs):\n",
        "        xs = torch.tensor([[a, b, p] for a, b in pairs], dtype=torch.long)\n",
        "        ys = torch.tensor([(a + b) % p for a, b in pairs], dtype=torch.long)\n",
        "        return xs, ys\n",
        "\n",
        "    train_x, train_y = pairs_to_tensors(train_pairs)\n",
        "    test_x, test_y = pairs_to_tensors(test_pairs)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# BASELINE: STANDARD TRANSFORMER (Nanda)\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model) # ABSOLUTE PE\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# THE MENACE: PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "\n",
        "        self.pos_angles = nn.Embedding(3, d_model) # ABSOLUTE PE (Phase Eq)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# OPTIMIZER ROUTER & TRAINING LOGIC\n",
        "# ==========================================\n",
        "def create_optimizer(model, lr, weight_decay, is_phase_model=False):\n",
        "    if not is_phase_model:\n",
        "        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
        "\n",
        "    decay_params, no_decay_params = [], []\n",
        "    protected_modules = ['tok_embed_real', 'tok_embed_imag', 'W_Q', 'W_K', 'W_V']\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad: continue\n",
        "        if any(p in name for p in protected_modules):\n",
        "            no_decay_params.append(param)\n",
        "        else:\n",
        "            decay_params.append(param)\n",
        "\n",
        "    return torch.optim.AdamW([\n",
        "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "    ], lr=lr, betas=(0.9, 0.98))\n",
        "\n",
        "def train_model(model, name, is_phase, train_x, train_y, test_x, test_y):\n",
        "    optimizer = create_optimizer(model, LR, WEIGHT_DECAY, is_phase)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')).item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\",\n",
        "                'te_loss': f\"{test_loss:.3f}\",\n",
        "                '|g|': f\"{grad_norm:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} GROKKED at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else \">35000\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(SEED)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: Standard vs. Phase Physics\")\n",
        "    print(f\"Modulus P={P} | d_model={D_MODEL} | wd={WEIGHT_DECAY} | lr={LR}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    models = [\n",
        "        (\"Standard Transformer\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM), False),\n",
        "        (\"Phase Transformer\", PhaseTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM), True)\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for name, model, is_phase in models:\n",
        "        model = model.to(DEVICE)\n",
        "        set_seed(SEED) # Reset seed before each run for fair init\n",
        "        results[name] = train_model(model, name, is_phase, tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"{'MODEL TOPOLOGY':<25} | {'EPOCHS TO GROK (>95% Acc)'}\")\n",
        "    print(\"-\" * 60)\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:<25} | {res['grok_epoch']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- THE PLOT ---\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    std_hist = results[\"Standard Transformer\"][\"history\"]\n",
        "    phase_hist = results[\"Phase Transformer\"][\"history\"]\n",
        "\n",
        "    # PLOT 1: Standard Memorization Trap\n",
        "    axs[0, 0].plot(std_hist[\"epochs\"], std_hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2)\n",
        "    axs[0, 0].plot(std_hist[\"epochs\"], std_hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2)\n",
        "    axs[0, 0].fill_between(std_hist[\"epochs\"], std_hist[\"train_acc\"], std_hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "    axs[0, 0].set_title(\"Standard Transformer (Abs PE)\\nThe Memorization Trap\", fontsize=14, pad=10)\n",
        "    axs[0, 0].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "    axs[0, 0].grid(True, alpha=0.3)\n",
        "    axs[0, 0].legend(loc=\"lower right\")\n",
        "\n",
        "    # PLOT 2: Phase Physics Bypass\n",
        "    axs[0, 1].plot(phase_hist[\"epochs\"], phase_hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2)\n",
        "    axs[0, 1].plot(phase_hist[\"epochs\"], phase_hist[\"test_acc\"], label=\"Test Acc\", color=\"#2ca02c\", linewidth=2)\n",
        "    axs[0, 1].fill_between(phase_hist[\"epochs\"], phase_hist[\"train_acc\"], phase_hist[\"test_acc\"], color='green', alpha=0.1)\n",
        "    axs[0, 1].set_title(\"Phase Transformer (|z|=1)\\nMemorization Physically Blocked\", fontsize=14, pad=10)\n",
        "    axs[0, 1].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "    axs[0, 1].grid(True, alpha=0.3)\n",
        "    axs[0, 1].legend(loc=\"lower right\")\n",
        "\n",
        "    # PLOT 3: Test Accuracy\n",
        "    axs[1, 0].plot(std_hist[\"epochs\"], std_hist[\"test_acc\"], label=\"Standard (Abs PE)\", color=\"#d62728\", linestyle=\"--\", alpha=0.8)\n",
        "    axs[1, 0].plot(phase_hist[\"epochs\"], phase_hist[\"test_acc\"], label=\"Phase (|z|=1)\", color=\"#2ca02c\", linewidth=2.5)\n",
        "    axs[1, 0].set_title(\"Test Accuracy Comparison\", fontsize=14, pad=10)\n",
        "    axs[1, 0].set_xlabel(\"Training Epochs\", fontsize=12)\n",
        "    axs[1, 0].set_ylabel(\"Test Accuracy\", fontsize=12)\n",
        "    axs[1, 0].grid(True, alpha=0.3)\n",
        "    axs[1, 0].legend(loc=\"lower right\")\n",
        "\n",
        "    # PLOT 4: Test Loss\n",
        "    axs[1, 1].plot(std_hist[\"epochs\"], std_hist[\"test_loss\"], label=\"Standard\", color=\"#d62728\", linestyle=\"--\", alpha=0.8)\n",
        "    axs[1, 1].plot(phase_hist[\"epochs\"], phase_hist[\"test_loss\"], label=\"Phase\", color=\"#2ca02c\", linewidth=2.5)\n",
        "    axs[1, 1].set_title(\"Test Loss Trajectory (Log Scale)\", fontsize=14, pad=10)\n",
        "    axs[1, 1].set_xlabel(\"Training Epochs\", fontsize=12)\n",
        "    axs[1, 1].set_ylabel(\"Cross Entropy Loss\", fontsize=12)\n",
        "    axs[1, 1].set_yscale(\"log\")\n",
        "    axs[1, 1].grid(True, alpha=0.3)\n",
        "    axs[1, 1].legend(loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "epx94UYLgDEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1.0\n",
        "EPOCHS = 40000           # Pushed slightly to ensure standard grokking finishes\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42                # Seed 42 usually groks around 25k-35k with correct betas\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# STANDARD TRANSFORMER\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim, normalize_hiddens=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.normalize_hiddens = normalize_hiddens # Apple-to-apples phase trick\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        if self.normalize_hiddens:\n",
        "            h = F.normalize(h, dim=-1) # Bounds vectors to the unit hypersphere\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        if self.normalize_hiddens:\n",
        "            h = F.normalize(h, dim=-1)\n",
        "\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        if self.normalize_hiddens:\n",
        "            h = F.normalize(h, dim=-1)\n",
        "\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# FOURIER INITIALIZATION (The Trojan Horse)\n",
        "# ==========================================\n",
        "def inject_fourier_bias(model, p):\n",
        "    \"\"\"\n",
        "    Injects the 5 key frequencies found by Nanda et al. directly into the\n",
        "    embedding matrix at initialization. The architecture remains exactly the same.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        key_freqs = [14, 35, 41, 42, 52]\n",
        "        # We overwrite the first 10 dimensions with perfect sines and cosines\n",
        "        for i, k in enumerate(key_freqs):\n",
        "            for x in range(p):\n",
        "                val = 2 * math.pi * k * x / p\n",
        "                model.tok_embed.weight[x, 2 * i] = math.cos(val)\n",
        "                model.tok_embed.weight[x, 2 * i + 1] = math.sin(val)\n",
        "\n",
        "# ==========================================\n",
        "# TRAINING LOGIC\n",
        "# ==========================================\n",
        "def train_model(model, name, train_x, train_y, test_x, test_y):\n",
        "    # Notice we reverted betas to default (0.9, 0.999). 0.98 kills momentum late in training!\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\",\n",
        "                'te_loss': f\"{test_loss:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else \">40000\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING: Inductive Bias vs FLOP-Matched Baselines\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    # 1. Standard Model\n",
        "    set_seed(SEED)\n",
        "    std_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM).to(DEVICE)\n",
        "    res_std = train_model(std_model, \"Standard\", tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # 2. Fourier Init Model (Identical FLOPs, just given the circle at step 1)\n",
        "    set_seed(SEED)\n",
        "    fourier_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM).to(DEVICE)\n",
        "    inject_fourier_bias(fourier_model, P)\n",
        "    res_fourier = train_model(fourier_model, \"Fourier Init\", tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # 3. Spherical Model (Your Phase block idea, but generalized to L2 norm, same FLOPs)\n",
        "    set_seed(SEED)\n",
        "    sphere_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, normalize_hiddens=True).to(DEVICE)\n",
        "    res_sphere = train_model(sphere_model, \"Spherical Norm\", tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for i, (name, res) in enumerate(zip([\"Standard\", \"Fourier Init\", \"Spherical Norm\"],\n",
        "                                        [res_std, res_fourier, res_sphere])):\n",
        "        hist = res[\"history\"]\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2)\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2)\n",
        "        axs[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "        axs[i].set_title(f\"{name} Transformer\\nGeneralized at: {res['grok_epoch']}\", fontsize=12)\n",
        "        axs[i].set_xlabel(\"Epochs\")\n",
        "        axs[i].set_ylabel(\"Accuracy\")\n",
        "        axs[i].grid(True, alpha=0.3)\n",
        "        axs[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CbZ_borZjiYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1.0\n",
        "EPOCHS = 40000\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# 1. STANDARD & SPHERICAL TRANSFORMER\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim, normalize_hiddens=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.normalize_hiddens = normalize_hiddens\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "def inject_fourier_bias(model, p):\n",
        "    with torch.no_grad():\n",
        "        key_freqs = [14, 35, 41, 42, 52]\n",
        "        for i, k in enumerate(key_freqs):\n",
        "            for x in range(p):\n",
        "                val = 2 * math.pi * k * x / p\n",
        "                model.tok_embed.weight[x, 2 * i] = math.cos(val)\n",
        "                model.tok_embed.weight[x, 2 * i + 1] = math.sin(val)\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE STRICT PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Real part of complex dot product is exact cosine similarity on the unit circle\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# TRAINING LOGIC\n",
        "# ==========================================\n",
        "def train_model(model, name, train_x, train_y, test_x, test_y):\n",
        "    # Standard AdamW used uniformly for fair landscape comparison\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else f\">{EPOCHS}\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING (2x2 Grid)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: The 4 Quadrants of Generalization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # 1. Standard Model (Baseline)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Standard\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)))\n",
        "\n",
        "    # 2. Fourier Init Model (Treasure Map)\n",
        "    set_seed(SEED)\n",
        "    f_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)\n",
        "    inject_fourier_bias(f_model, P)\n",
        "    models.append((\"Fourier Init\", f_model))\n",
        "\n",
        "    # 3. Spherical Norm (L2 Straitjacket)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Spherical Norm\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, True)))\n",
        "\n",
        "    # 4. Phase Transformer (Complex Geometry Straitjacket)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Strict Phase (|z|=1)\", PhaseTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM)))\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models:\n",
        "        model = model.to(DEVICE)\n",
        "        results[name] = train_model(model, name, tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # --- PLOTTING 2x2 GRID ---\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, (name, res) in enumerate(results.items()):\n",
        "        hist = res[\"history\"]\n",
        "\n",
        "        # Determine if it \"grokked\" (has a massive train/test gap) or \"just learned\" (lines hug each other)\n",
        "        # We shade the gap red to highlight the memorization phase.\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "        axs[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "        title_text = f\"{name} Transformer\\nGeneralized at: {res['grok_epoch']}\"\n",
        "        axs[i].set_title(title_text, fontsize=14, pad=10)\n",
        "        axs[i].set_xlabel(\"Epochs\", fontsize=12)\n",
        "        axs[i].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "        axs[i].grid(True, alpha=0.3)\n",
        "        axs[i].legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2LHsyp83mZxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 0.0\n",
        "EPOCHS = 40000\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# 1. STANDARD & SPHERICAL TRANSFORMER\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim, normalize_hiddens=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.normalize_hiddens = normalize_hiddens\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "def inject_fourier_bias(model, p):\n",
        "    with torch.no_grad():\n",
        "        key_freqs = [14, 35, 41, 42, 52]\n",
        "        for i, k in enumerate(key_freqs):\n",
        "            for x in range(p):\n",
        "                val = 2 * math.pi * k * x / p\n",
        "                model.tok_embed.weight[x, 2 * i] = math.cos(val)\n",
        "                model.tok_embed.weight[x, 2 * i + 1] = math.sin(val)\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE STRICT PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Real part of complex dot product is exact cosine similarity on the unit circle\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# TRAINING LOGIC\n",
        "# ==========================================\n",
        "def train_model(model, name, train_x, train_y, test_x, test_y):\n",
        "    # Standard AdamW used uniformly for fair landscape comparison\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(EPOCHS), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ {name} generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else f\">{EPOCHS}\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING (2x2 Grid)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: The 4 Quadrants of Generalization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # 1. Standard Model (Baseline)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Standard\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)))\n",
        "\n",
        "    # 2. Fourier Init Model (Treasure Map)\n",
        "    set_seed(SEED)\n",
        "    f_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)\n",
        "    inject_fourier_bias(f_model, P)\n",
        "    models.append((\"Fourier Init\", f_model))\n",
        "\n",
        "    # 3. Spherical Norm (L2 Straitjacket)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Spherical Norm\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, True)))\n",
        "\n",
        "    # 4. Phase Transformer (Complex Geometry Straitjacket)\n",
        "    set_seed(SEED)\n",
        "    models.append((\"Strict Phase (|z|=1)\", PhaseTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM)))\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models:\n",
        "        model = model.to(DEVICE)\n",
        "        results[name] = train_model(model, name, tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # --- PLOTTING 2x2 GRID ---\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, (name, res) in enumerate(results.items()):\n",
        "        hist = res[\"history\"]\n",
        "\n",
        "        # Determine if it \"grokked\" (has a massive train/test gap) or \"just learned\" (lines hug each other)\n",
        "        # We shade the gap red to highlight the memorization phase.\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "        axs[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "        axs[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "        title_text = f\"{name} Transformer\\nGeneralized at: {res['grok_epoch']}\"\n",
        "        axs[i].set_title(title_text, fontsize=14, pad=10)\n",
        "        axs[i].set_xlabel(\"Epochs\", fontsize=12)\n",
        "        axs[i].set_ylabel(\"Accuracy\", fontsize=12)\n",
        "        axs[i].grid(True, alpha=0.3)\n",
        "        axs[i].legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P1_ev70NsVtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "EPOCHS = 40000\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# THE STRICT PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        # ---> THE ONLY PLACES MAGNITUDE CAN GROW <---\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# SURGICAL OPTIMIZER ROUTING\n",
        "# ==========================================\n",
        "def train_surgical_phase_model(model, train_x, train_y, test_x, test_y):\n",
        "    # Route Weight Decay ONLY to the exit pipeline\n",
        "    decay_params = []\n",
        "    no_decay_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad: continue\n",
        "        # Target the final linear layers where magnitudes can explode\n",
        "        if 'bridge' in name or 'unembed' in name:\n",
        "            decay_params.append(param)\n",
        "        else:\n",
        "            no_decay_params.append(param)\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": decay_params, \"weight_decay\": 1.0},\n",
        "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "    ], lr=LR, betas=(0.9, 0.999))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(EPOCHS), desc=\"Training Phase (Targeted WD)\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == EPOCHS - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\n⚡ Surgical Phase Model generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else f\">{EPOCHS}\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TARGETED WEIGHT DECAY: Phase Transformer\")\n",
        "    print(\"WD=1.0 on [Bridge, Unembed], WD=0.0 on [Embeds, Attn, FFN]\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PhaseTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM).to(DEVICE)\n",
        "    res = train_surgical_phase_model(model, tr_x, tr_y, te_x, te_y)\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    hist = res[\"history\"]\n",
        "\n",
        "    # Plot 1: Accuracy\n",
        "    ax1.plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "    ax1.plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "    ax1.fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "    ax1.set_title(f\"Targeted WD Phase Transformer\\nGeneralized at: {res['grok_epoch']}\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Epochs\", fontsize=12)\n",
        "    ax1.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "\n",
        "    # Plot 2: Loss\n",
        "    ax2.plot(hist[\"epochs\"], hist[\"train_loss\"], label=\"Train Loss\", color=\"#1f77b4\", linewidth=2.5)\n",
        "    ax2.plot(hist[\"epochs\"], hist[\"test_loss\"], label=\"Test Loss\", color=\"#d62728\", linewidth=2.5)\n",
        "    ax2.set_title(\"Cross-Entropy Loss (Log Scale)\", fontsize=14)\n",
        "    ax2.set_xlabel(\"Epochs\", fontsize=12)\n",
        "    ax2.set_ylabel(\"Loss\", fontsize=12)\n",
        "    ax2.set_yscale(\"log\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PC60LoVSvxk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}