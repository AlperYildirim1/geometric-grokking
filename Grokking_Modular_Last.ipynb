{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1SVp6ag1XCaM00uysczxbaEslzEmxS0FV",
      "authorship_tag": "ABX9TyNP9TxoDQbtNqT2fMX4b52c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/geometric-grokking/blob/main/Grokking_Modular_Last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs_detailed'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "P = 113\n",
        "FRAC_TRAIN = 0.3\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1.0\n",
        "LOG_EVERY = 200\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# ==========================================\n",
        "# BULLETPROOF DETERMINISM\n",
        "# ==========================================\n",
        "def set_seed(seed):\n",
        "    \"\"\"Locks down all sources of randomness for 100% reproducibility.\"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # Required for strict cuDNN determinism\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "# ==========================================\n",
        "# DATASET GENERATION\n",
        "# ==========================================\n",
        "def make_dataset(p, frac_train, seed=42):\n",
        "    set_seed(seed) # Ensure dataset split is also deterministic per seed\n",
        "    rng = random.Random(seed)\n",
        "    all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
        "    rng.shuffle(all_pairs)\n",
        "\n",
        "    n_train = int(len(all_pairs) * frac_train)\n",
        "    train_x = torch.tensor([[a, b, p] for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    train_y = torch.tensor([(a + b) % p for a, b in all_pairs[:n_train]], dtype=torch.long)\n",
        "    test_x = torch.tensor([[a, b, p] for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    test_y = torch.tensor([(a + b) % p for a, b in all_pairs[n_train:]], dtype=torch.long)\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "# ==========================================\n",
        "# 1. STANDARD & SPHERICAL TRANSFORMER\n",
        "# ==========================================\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim, normalize_hiddens=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.normalize_hiddens = normalize_hiddens\n",
        "\n",
        "        self.tok_embed = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_embed = nn.Embedding(3, d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.mlp_in = nn.Linear(d_model, mlp_dim)\n",
        "        self.mlp_out = nn.Linear(mlp_dim, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        h = self.tok_embed(x) + self.pos_embed(pos)\n",
        "\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        Q = self.W_Q(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        K = self.W_K(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        V = self.W_V(h).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        attn_out = self.W_O((F.softmax(scores, dim=-1) @ V).transpose(1, 2).contiguous().view(B, L, self.d_model))\n",
        "\n",
        "        h = h + attn_out\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        h = h + self.mlp_out(F.relu(self.mlp_in(h)))\n",
        "        if self.normalize_hiddens: h = F.normalize(h, dim=-1)\n",
        "\n",
        "        return self.unembed(h[:, 2, :])\n",
        "\n",
        "def inject_fourier_bias(model, p):\n",
        "    with torch.no_grad():\n",
        "        key_freqs = [14, 35, 41, 42, 52]\n",
        "        for i, k in enumerate(key_freqs):\n",
        "            for x in range(p):\n",
        "                val = 2 * math.pi * k * x / p\n",
        "                model.tok_embed.weight[x, 2 * i] = math.cos(val)\n",
        "                model.tok_embed.weight[x, 2 * i + 1] = math.sin(val)\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE STRICT PHASE TRANSFORMER (|z|=1)\n",
        "# ==========================================\n",
        "def strictly_phase(z, eps=1e-6):\n",
        "    return z / (z.abs() + eps)\n",
        "\n",
        "class StrictComplexLinear(nn.Module):\n",
        "    def __init__(self, d_in, d_out, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.W_real = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_imag = nn.Linear(d_in, d_out, bias=False)\n",
        "        nn.init.normal_(self.W_real.weight, std=init_scale)\n",
        "        nn.init.normal_(self.W_imag.weight, std=init_scale)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.complex(\n",
        "            self.W_real(z.real) - self.W_imag(z.imag),\n",
        "            self.W_real(z.imag) + self.W_imag(z.real)\n",
        "        )\n",
        "\n",
        "class PhaseAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.W_Q = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_K = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_V = StrictComplexLinear(d_model, d_model)\n",
        "        self.W_O = StrictComplexLinear(d_model, d_model, init_scale=0.001)\n",
        "\n",
        "    def forward(self, z):\n",
        "        B, L, D = z.shape\n",
        "        q = strictly_phase(self.W_Q(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        k = strictly_phase(self.W_K(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        v = strictly_phase(self.W_V(z)).view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.conj().transpose(-2, -1)).real * (self.d_head ** -0.5)\n",
        "        attn_out = strictly_phase(F.softmax(scores, dim=-1).to(v.dtype) @ v)\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        return self.W_O(attn_out)\n",
        "\n",
        "class PhaseFFN(nn.Module):\n",
        "    def __init__(self, d_model, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, d_model)\n",
        "        )\n",
        "        nn.init.zeros_(self.net[2].weight)\n",
        "        nn.init.zeros_(self.net[2].bias)\n",
        "\n",
        "    def forward(self, z):\n",
        "        features = torch.cat([z.real, z.imag], dim=-1)\n",
        "        angles = self.net(features)\n",
        "        return torch.exp(1j * angles)\n",
        "\n",
        "class PhaseTransformer(nn.Module):\n",
        "    def __init__(self, p, d_model, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.tok_embed_real = nn.Embedding(p + 1, d_model)\n",
        "        self.tok_embed_imag = nn.Embedding(p + 1, d_model)\n",
        "        self.pos_angles = nn.Embedding(3, d_model)\n",
        "        nn.init.zeros_(self.pos_angles.weight)\n",
        "\n",
        "        self.attn = PhaseAttention(d_model, num_heads)\n",
        "        self.ffn = PhaseFFN(d_model, mlp_dim)\n",
        "\n",
        "        self.bridge = nn.Linear(d_model * 2, d_model)\n",
        "        self.unembed = nn.Linear(d_model, p, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        z = strictly_phase(torch.complex(self.tok_embed_real(x), self.tok_embed_imag(x)))\n",
        "        pos = torch.arange(L, device=x.device).unsqueeze(0)\n",
        "        z = strictly_phase(z * torch.exp(1j * self.pos_angles(pos)))\n",
        "\n",
        "        z = strictly_phase(z + self.attn(z))\n",
        "        z = strictly_phase(z * self.ffn(z))\n",
        "\n",
        "        h_real = self.bridge(torch.cat([z.real, z.imag], dim=-1))\n",
        "        return self.unembed(h_real[:, 2, :])\n",
        "\n",
        "# ==========================================\n",
        "# TRAINING LOGIC\n",
        "# ==========================================\n",
        "def train_model(model, name, train_x, train_y, test_x, test_y, epochs):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_x, train_y = train_x.to(DEVICE), train_y.to(DEVICE)\n",
        "    test_x, test_y = test_x.to(DEVICE), test_y.to(DEVICE)\n",
        "\n",
        "    grok_epoch = None\n",
        "    history = {'epochs': [], 'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    pbar = tqdm(range(epochs), desc=f\"Training {name}\")\n",
        "\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        logits = model(train_x)\n",
        "        loss = criterion(logits, train_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % LOG_EVERY == 0 or epoch == epochs - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_acc = (logits.argmax(-1) == train_y).float().mean().item()\n",
        "                test_logits = model(test_x)\n",
        "                test_loss = criterion(test_logits, test_y).item()\n",
        "                test_acc = (test_logits.argmax(-1) == test_y).float().mean().item()\n",
        "\n",
        "            history['epochs'].append(epoch)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['test_acc'].append(test_acc)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['test_loss'].append(test_loss)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'tr_acc': f\"{train_acc:.3f}\",\n",
        "                'te_acc': f\"{test_acc:.3f}\"\n",
        "            })\n",
        "\n",
        "            if grok_epoch is None and test_acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                tqdm.write(f\"\\nâš¡ {name} generalized at epoch {epoch}! (test_acc={test_acc:.4f})\")\n",
        "\n",
        "    return {\"grok_epoch\": grok_epoch if grok_epoch else f\">{epochs}\", \"history\": history}\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & PLOTTING (1x4, 2x2, and Zoomed Grids)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GROKKING LITMUS TEST: The 4 Quadrants of Generalization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Loop through seeds 1 to 5 automatically\n",
        "    for SEED in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "        print(f\"\\n\\n{'=' * 40}\")\n",
        "        print(f\"ðŸš€ STARTING RUN FOR SEED {SEED} ðŸš€\")\n",
        "        print(f\"{'=' * 40}\\n\")\n",
        "\n",
        "        # 1. Generate Dataset Deterministically\n",
        "        tr_x, tr_y, te_x, te_y = make_dataset(P, FRAC_TRAIN, seed=SEED)\n",
        "\n",
        "        # Format: (Name, Model, Epochs)\n",
        "        models = []\n",
        "\n",
        "        # 2. Standard Model (Baseline) - 60k epochs\n",
        "        set_seed(SEED)\n",
        "        models.append((\"Standard\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False), 60000))\n",
        "\n",
        "        # 3. Fourier Init Model (Treasure Map) - 40k epochs\n",
        "        set_seed(SEED)\n",
        "        f_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, False)\n",
        "        inject_fourier_bias(f_model, P)\n",
        "        models.append((\"Fourier Init\", f_model, 40000))\n",
        "\n",
        "        # 4. Spherical Norm (L2 Straitjacket) - Learns instantly, stops at 10k\n",
        "        set_seed(SEED)\n",
        "        models.append((\"Spherical Norm\", StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, True), 10000))\n",
        "\n",
        "        # 5. The Ultimate Combo: Fourier Init + Spherical Norm - stops at 10k\n",
        "        set_seed(SEED)\n",
        "        combo_model = StandardTransformer(P, D_MODEL, NUM_HEADS, MLP_DIM, True)\n",
        "        inject_fourier_bias(combo_model, P)\n",
        "        models.append((\"Fourier + Spherical\", combo_model, 10000))\n",
        "\n",
        "        results = {}\n",
        "        for name, model, train_epochs in models:\n",
        "            model = model.to(DEVICE)\n",
        "            results[name] = train_model(model, name, tr_x, tr_y, te_x, te_y, train_epochs)\n",
        "\n",
        "            # Save logs to Drive\n",
        "            safe_name = name.replace(\" \", \"_\").replace(\"(|z|=1)\", \"z1\")\n",
        "            save_path = os.path.join(SAVE_DIR, f\"{safe_name}_seed{SEED}.json\")\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(results[name][\"history\"], f)\n",
        "            print(f\"Saved {name} logs to {save_path}\")\n",
        "\n",
        "        # ==========================================\n",
        "        # --- PLOTTING 1x4 HORIZONTAL GRID ---\n",
        "        # ==========================================\n",
        "        fig_1x4, axs_1x4 = plt.subplots(1, 4, figsize=(24, 5))\n",
        "        axs_1x4 = axs_1x4.flatten()\n",
        "\n",
        "        for i, (name, res) in enumerate(results.items()):\n",
        "            hist = res[\"history\"]\n",
        "\n",
        "            axs_1x4[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "            axs_1x4[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "            axs_1x4[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "            title_text = f\"{name}\\nGeneralized at: {res['grok_epoch']}\"\n",
        "            axs_1x4[i].set_title(title_text, fontsize=16, pad=10)\n",
        "            axs_1x4[i].set_xlabel(\"Epochs\", fontsize=14)\n",
        "            axs_1x4[i].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "            axs_1x4[i].set_xlim(0, 60000)  # <--- LOCKED X-AXIS\n",
        "            axs_1x4[i].tick_params(axis='both', which='major', labelsize=12)\n",
        "            axs_1x4[i].grid(True, alpha=0.3)\n",
        "            axs_1x4[i].legend(loc=\"lower right\", fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_path_1x4 = os.path.join(SAVE_DIR, f\"grokking_1x4_grid_seed{SEED}.png\")\n",
        "        plt.savefig(plot_path_1x4, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… Saved 1x4 plot to: {plot_path_1x4}\")\n",
        "        plt.close(fig_1x4) # Close to prevent overlapping plots\n",
        "\n",
        "        # ==========================================\n",
        "        # --- PLOTTING 2x2 SQUARE GRID ---\n",
        "        # ==========================================\n",
        "        fig_2x2, axs_2x2 = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        axs_2x2 = axs_2x2.flatten()\n",
        "\n",
        "        for i, (name, res) in enumerate(results.items()):\n",
        "            hist = res[\"history\"]\n",
        "\n",
        "            axs_2x2[i].plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "            axs_2x2[i].plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "            axs_2x2[i].fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "            title_text = f\"{name}\\nGeneralized at: {res['grok_epoch']}\"\n",
        "            axs_2x2[i].set_title(title_text, fontsize=16, pad=10)\n",
        "            axs_2x2[i].set_xlabel(\"Epochs\", fontsize=14)\n",
        "            axs_2x2[i].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "            axs_2x2[i].set_xlim(0, 60000)  # <--- LOCKED X-AXIS\n",
        "            axs_2x2[i].tick_params(axis='both', which='major', labelsize=12)\n",
        "            axs_2x2[i].grid(True, alpha=0.3)\n",
        "            axs_2x2[i].legend(loc=\"lower right\", fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_path_2x2 = os.path.join(SAVE_DIR, f\"grokking_2x2_grid_seed{SEED}.png\")\n",
        "        plt.savefig(plot_path_2x2, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ… Saved 2x2 plot to: {plot_path_2x2}\")\n",
        "\n",
        "        # Show the 2x2 in the notebook output\n",
        "        plt.show()\n",
        "\n",
        "        # ==========================================\n",
        "        # --- PLOTTING 1x1 ZOOMED GRIDS (SPHERICAL ONLY) ---\n",
        "        # ==========================================\n",
        "        for name, res in results.items():\n",
        "            if name in [\"Spherical Norm\", \"Fourier + Spherical\"]:\n",
        "                fig_zoom, ax_zoom = plt.subplots(figsize=(8, 6))\n",
        "                hist = res[\"history\"]\n",
        "\n",
        "                ax_zoom.plot(hist[\"epochs\"], hist[\"train_acc\"], label=\"Train Acc\", color=\"#1f77b4\", linewidth=2.5)\n",
        "                ax_zoom.plot(hist[\"epochs\"], hist[\"test_acc\"], label=\"Test Acc\", color=\"#d62728\", linewidth=2.5)\n",
        "                ax_zoom.fill_between(hist[\"epochs\"], hist[\"train_acc\"], hist[\"test_acc\"], color='red', alpha=0.1)\n",
        "\n",
        "                title_text = f\"{name} (Zoomed 5k)\\nGeneralized at: {res['grok_epoch']}\"\n",
        "                ax_zoom.set_title(title_text, fontsize=16, pad=10)\n",
        "                ax_zoom.set_xlabel(\"Epochs\", fontsize=14)\n",
        "                ax_zoom.set_ylabel(\"Accuracy\", fontsize=14)\n",
        "                ax_zoom.set_xlim(0, 5000)  # <--- ZOOMED X-AXIS\n",
        "                ax_zoom.tick_params(axis='both', which='major', labelsize=12)\n",
        "                ax_zoom.grid(True, alpha=0.3)\n",
        "                ax_zoom.legend(loc=\"lower right\", fontsize=12)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                safe_name = name.replace(\" \", \"_\").replace(\"(|z|=1)\", \"z1\")\n",
        "                plot_path_zoom = os.path.join(SAVE_DIR, f\"grokking_zoomed_{safe_name}_seed{SEED}.png\")\n",
        "                plt.savefig(plot_path_zoom, dpi=300, bbox_inches='tight')\n",
        "                print(f\"âœ… Saved Zoomed plot to: {plot_path_zoom}\")\n",
        "                plt.close(fig_zoom)"
      ],
      "metadata": {
        "id": "tWDRCTaNPA7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs_detailed'\n",
        "model_names = [\"Standard\", \"Fourier_Init\", \"Spherical_Norm\", \"Fourier_+_Spherical\"]\n",
        "\n",
        "results_data = []\n",
        "\n",
        "print(\"Analyzing grokking moments across all seeds...\\n\")\n",
        "\n",
        "for model in model_names:\n",
        "    files = glob.glob(f\"{SAVE_DIR}/{model}_seed*.json\")\n",
        "    if not files:\n",
        "        continue\n",
        "\n",
        "    grok_epochs = []\n",
        "    did_not_grok_count = 0\n",
        "\n",
        "    for f in files:\n",
        "        data = json.load(open(f))\n",
        "        test_accs = data[\"test_acc\"]\n",
        "        epochs = data[\"epochs\"]\n",
        "\n",
        "        # Find the first epoch where test_acc > 0.95\n",
        "        grok_epoch = None\n",
        "        for epoch, acc in zip(epochs, test_accs):\n",
        "            if acc > 0.95:\n",
        "                grok_epoch = epoch\n",
        "                break\n",
        "\n",
        "        if grok_epoch is not None:\n",
        "            grok_epochs.append(grok_epoch)\n",
        "        else:\n",
        "            did_not_grok_count += 1\n",
        "\n",
        "    if grok_epochs:\n",
        "        results_data.append({\n",
        "            \"Model\": model.replace(\"_\", \" \"),\n",
        "            \"Seeds Run\": len(files),\n",
        "            \"Successful Groks\": len(grok_epochs),\n",
        "            \"Earliest Grok\": np.min(grok_epochs),\n",
        "            \"Latest Grok\": np.max(grok_epochs),\n",
        "            \"Average Grok\": int(np.mean(grok_epochs)),\n",
        "            \"Std Dev\": int(np.std(grok_epochs))\n",
        "        })\n",
        "\n",
        "if results_data:\n",
        "    # Use Pandas to print a clean, paper-ready table\n",
        "    df = pd.DataFrame(results_data)\n",
        "    print(df.to_markdown(index=False))\n",
        "else:\n",
        "    print(\"No log files found yet. Make sure training has finished!\")"
      ],
      "metadata": {
        "id": "6Oa4oUsjhSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import subprocess\n",
        "import datetime\n",
        "\n",
        "# Make sure the directory exists\n",
        "SAVE_DIR = '/content/drive/MyDrive/grokking_logs'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "env_log_path = os.path.join(SAVE_DIR, \"environment_info.txt\")\n",
        "\n",
        "with open(env_log_path, \"w\") as f:\n",
        "    f.write(f\"--- Grokking Experiment Environment Log ---\\n\")\n",
        "    f.write(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    # 1. Python Version\n",
        "    f.write(\"=== Python Version ===\\n\")\n",
        "    f.write(sys.version + \"\\n\\n\")\n",
        "\n",
        "    # 2. PyTorch & GPU Info\n",
        "    f.write(\"=== PyTorch & CUDA ===\\n\")\n",
        "    f.write(f\"PyTorch Version: {torch.__version__}\\n\")\n",
        "    f.write(f\"CUDA Available: {torch.cuda.is_available()}\\n\")\n",
        "    if torch.cuda.is_available():\n",
        "        f.write(f\"CUDA Built-in Version: {torch.version.cuda}\\n\")\n",
        "        f.write(f\"cuDNN Version: {torch.backends.cudnn.version()}\\n\")\n",
        "        f.write(f\"GPU Model: {torch.cuda.get_device_name(0)}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    # 3. Full NVIDIA-SMI Output\n",
        "    f.write(\"=== NVIDIA-SMI ===\\n\")\n",
        "    try:\n",
        "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
        "        f.write(nvidia_smi + \"\\n\")\n",
        "    except Exception as e:\n",
        "        f.write(f\"Could not retrieve nvidia-smi: {e}\\n\\n\")\n",
        "\n",
        "    # 4. Pip Freeze (All installed libraries)\n",
        "    f.write(\"=== Installed PIP Packages ===\\n\")\n",
        "    try:\n",
        "        pip_freeze = subprocess.check_output(\"pip freeze\", shell=True).decode()\n",
        "        f.write(pip_freeze + \"\\n\")\n",
        "    except Exception as e:\n",
        "        f.write(f\"Could not retrieve pip freeze: {e}\\n\")\n",
        "\n",
        "print(f\"âœ… Environment successfully logged to: {env_log_path}\")"
      ],
      "metadata": {
        "id": "SiNdMfEpJwyv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}